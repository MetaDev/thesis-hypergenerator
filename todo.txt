increase number of children to 3

test gmm1 test with pomegranate

add own gmm class wrapper around gmm from 3 different gmm libraries
add pomegranate for weights and decay (to maybe solve MO)
initialise weights,covars and priors such that the gmm fits a unifrom distr in the given search space

learn to fit surface according to ratio in a polygon

find out why alignment is so hard to learn in gmm2
plot fitness seperately, fitness func is maybe not correct, try to visualise it

train model p(c1|c0 p)
->learn full distribution p(c)

refactor search space such that children can be made dependent on children
ordering of children-> node should possibly have multiple parents but (dag, check for this)
->use reactive programming

sampling should be possible in a grid for the first training step, refactor at the same time as to generate


in training utility, a method sould receive the samples, and the fitness function and return
the fitness values and according data used to calculate it
also an argument for fitness threshold, fitness target and whether you're looking to minimize or maximize the

rename learning utility as GMM extension



ask writer gmr for references on the methods used in condition and marginal

better print of node sample

learning optimisation
-------------------------
use feature extraction to optmize learning, but each feature should be able to be reconverted

use BIC to find optimal number of clusters

normalise the data you train on [0,1]

id hyperparameters for over/underfitting

make interface for training, train in batches by sampling from the previous model in the next
only expose hyperparamters that could influence over/under fitting (to be determined by experimentation)

learn(n_sample_barhces,n_batches,child_attributes,parent_attributes,fitness_funcs, GMM_hyperparams)

Note: larger batches will mean less overfitting because the model will train on more varied data
smaller batches but more number of batches will more overfit, reducing models expressivens but increasing average fitness faster


training should be done on scaled data
given only a set of attributes to train on and fitness function on the attribute vector for the weight of the samples

check if cross validation can benefit the system

experiment weighted samples vs slicing and  data or show that data with 0 weight is equal to sliced data

find attributes in children that don't interfere with fitness (can be omitted from training) (automatic feature extraction)
or attributes independent of others regardin the fitness (covariance analysis) (they can be trained seperately)(dim reduciton)

or even better (and in the case of child-child training): for large number of chidlren, deduce hierarchical independence between children (using clustering), this will return cluster label or each sample, which can be used to slice data accordingly for training
e.g. tables far apart will not likely collide
and train in a hierachical divided search space (naive bayes?, divide search space in n clusters)

train gaussian mixture using pomegranate (mixture of multivariate gaussian) (added benefit of decay, priority based multi-objective) also faster, correct see http://homes.cs.washington.edu/~jmschr/lectures/pomegranate.html


sampling optimisation
----------------------------


sampling would be greatly improved, and thus also the learning
if I would sample more (calculate fitness sample value pairs) on variables defined stochastic
even better would be to do that proportional to it's stochasity, this would be overfitting though

extend search space distribution with sample(n_samples), to generate multiple samples from the search space at once, this is an optimisation

sample in batches, resampling is done from trained model

check if data augmentation helps, extra samples where the fitness function varies a lot between close points
find high uncertainty in search space, especially around the borders, this does influence the search space (unintended complexity)

deterministic/stochastic sampling with stochastic mesh grid tree search

visualisation optimisation
----------------------------------------

could add fitness in representative range visualisation (color line in gradient reflecting each value avg fitness)

when visualising the plot should show also completely the ellipses of the distributions, now they are cut off

plot, during training, the fitness histogram (avg, var), and the expressiveness (the ratio of the interval you're still reaching, or the amount of variance)

implement method to cluster (unweighted )samples in X clusters and generate N samples from each cluster
to visualise as a way to show expressive range


code clean up
----------------------

add enum to know if stochastif is discrete or continuous

remove variable classes from search_space module

change name markov tree node

add type checking in python, seriously, where necessary (see package in bookmark thesis)
Types as annotations


examples
-------------------------

different model and constraints: generate chamber in a house, this is a shape->shape relation

see paper open world layour for coffee bar exmaple

test learning with larger hierachy -> 2 level constraints (room->table_chairs)

do the plates on a table example for multiple fitness

do an example of room generation, where the fitness can be the validity of a polygon (non-convex)

evaluation
------------------
influence of model prior (hierachical relations) on learning and fitness (hierachical relations)


sibling fitness
---------------
1. order siblings based on pair-wise fitness (weight function for samples)

2. train HMM on sequence of (parent,child1),(parent,child2), to be able to calculate conditional distr
afterwards

2.
construct seperate models for each number of children if trained on (ordered) sibling data and their fitness that is calculated between siblings (tree structure)

and train a model seperately on the structure of the network, using only (parent_n_siblings, n_siblings)->fitness to generate (conditional) n_siblings, than use training data to estimate optimal distribution for the number of children ()

3.
or train model on the joint of (nr_siblings, index (fixed order, not index from samples)) and parent  using pomegranata (use rejection sampling for sampling, or better MCMC (PyMC!))

4. for a large range in the number of children (for example a large area where trees are placed but the area can be both dense or open)-> seperate models unfeasible, fit distribution for a markov chain between ordered siblings (using gmm)

or better with a Hidden markov model using the children attributes as sequences
the hmm library is either hmmlearn or bnpy (use weighted sampels), propably won't integrate well with hierarchical model, set number of states the max(n_children) initialise transation distr according to specification in search space if the distr of n_children should not be learned

how do we condition on the parent state, convert the model to pymc and set the first state to observed. we could add the parent to the start of the sequence, but this would reduce our hierarchical model to a single long chain, not if we can condition on the first state




all papers in downloads->put in mendeley



2) model pokemon world probabilistically
    find top down constraints
find conditional distributions reflecting the number of children in the hierarchy conditional on the fitness




