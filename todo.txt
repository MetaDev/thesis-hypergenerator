

todo evaluation and test
--------------------------


fix that the last model is replaced by the worse model

do tests by hand

run test and save results to file (csv) with parameters as columns and results next to it


//sunday


write methodology


add outlier removal from the data formatting for training

implement convex polygon

reachability is implemented as the closest node to the target that can be reached from the start

//rerun tests with and without outlier removal





//sunday

write evaluation



evaluation and tests
-------------------------------

evaluation, the type of metrics and parameters that will be varied
3 models for scenes, constraints and search space

2. an outdoor scene (park, river, trees, benches, paths), trees should not be line of sight with benches and river
3. an indoor scene, ground floor of a house with different rooms: carpets, tables, chairs, plates

compare influence of dimensionality of fitness

compare the combination of several variables (like rotation and position) into the
joint distribution

compare different ways of combining fitness values either multiply, average, other more complex functions, (the order of a functions is already a way of changing this function) so different values for the order can be tested as well

show that PCA/LDA for dim reduction doesn't work

compare all possible parameter combinations possible (gridsearch) in the learning method

by training only on parent->child and increasing sibling order the scaling properties in dimensionality can be shown, in terms of necessary data, performance

compare with different amounts of data

evaluate with and without poisson sampling

compare differen configurations of sibling order learning and marginalisation

compare different values of capping the fitness

compare different values of conditions for the fitness in regression

compare weighted samples wrt regression

---for each use-case:

evaluate performance and expressiveness

calculate expressiveness by counting a lot of points in trained and non trained distributions

emperical performance is the average fitness (without order)
emperical expressiveness is the weighted variance of variables

an exact visualisation of the expressivenss can be done using grpags (1D and 2D)

visualise this performance and expressiveness over consecutive runs to show convergence


seperately the training itself can be evaluated

MSE for regression
MSE likelihood for weighted sampling




extra fitness funcs

reachability

learn to fit surface according to surface ratio in a polygon

find out why alignment is so hard to learn in gmm2

plot fitness seperately, fitness func is maybe not correct, try to visualise it

visualisation is part of the model, the mapping
add visualisation to fitness model as well

factory method to visualise fitness functions


writing
------------------
start writing methodology

discuss truncuatedsampling vs truncuated learning, performance vs precision (overfitting)

research:

Neufert constraints

research question:
How to optimise the search space of a procedural content generator, a priori defined by bottom up constraints and a posteriori by its top-down constraints, without negatively impacting its expressiveness?



learning optimisation
-------------------------
init gmm uniformly like gmr

add weigthed average as way to combine fitness

instead of multiplying the multiple fitness functions -> dim reduction


only assign sibling conditionallity if the found distribution is above a certain order of quality (in expressiveness)
or if higher order distribution is of lower quality, only train until n children and assign variables

evaluate quality of learned model, MSE form random points with high fitness

increase learning and sampling robustness by using k-fold cross validation.
wich would result in a mixture of GMM's with uniform weights to sample from.
or blend different mixtures together
this would increase the sampling time, but reduce the accidental bias


sampling optimisation
----------------------------

allowing constraints between differently named children
for example to align different types of children, this would imply a new order on these children
which should be respected when generating
or this order could just be set a priori and checked for correctness
this would allow to condition a childs variable on multiple other objects both it's ancestor as other siblings of
any class, each constraint will have to define which classes are conditioned on

allow encapsulation of a model into a node
this would allow for relations between variables of a node (now the definition of variable and node blurs togehter)
as a node is able to contain no variables but multiple nodes with their own variables
these variables however fall under the namespace of the encapsulating node

to sample from learned distribution using truncuated normal sampling

in your initial sampling you can always make the trade-off between expressiveness or exploration (search for diverse points)
and fitness or exploitation (search for best points)

conditioning is at the core of the sampling algorithm so that should be optimised

remove deterministic affine transformation chain from the nodesample class,
just sample the nodes and use the original node to calculate the relative values needed for the mapping

convert to more functional structure using rx, see rx_test
->use reactive programming, which makes it an data flow graph,
->make it possible to generate n samples, by creating a tree where each parent has max N_children subscriber nodes
->when sampling (2,5) number of children and 3 is calculated only send data to 3 out of children
->connect every node to an ouput node where all generated samples are captured

sample in batches using buffer of rx and train on each seperate buffer, afterwards evaluate each trained model
on average fitness and expressiveness

check which gmm samples the fastest

sampling would be greatly improved, and thus also the learning
if I would sample more (calculate fitness sample value pairs) on variables defined stochastic
even better would be to do that proportional to it's stochasity, this would be overfitting though

extend search space distribution with sample(n_samples), to generate multiple samples from the search space at once, this is an optimisation

sample in batches, resampling is done from trained model

check if data augmentation helps, extra samples where the fitness function varies a lot between close points
find high uncertainty in search space, especially around the borders, this does influence the search space (unintended complexity)

deterministic/stochastic sampling with stochastic mesh grid tree search

visualisation optimisation
----------------------------------------

calculate the surface difference between search space (rectangle) and a 99% confidence interval ellips

visualise search space of a variable, a rectangle or line for unifrom and ellipses or normal for gmm
add a method to the search space to visualise any variable by name

could add fitness in representative range visualisation (color line in gradient reflecting each value avg fitness)

when visualising the plot should show also completely the ellipses of the distributions, now they are cut off

plot, during training, the fitness histogram (avg, var), and the expressiveness (the ratio of the interval you're still reaching, or the amount of variance)

implement method to cluster (unweighted )samples in X clusters and generate N samples from each cluster
to visualise as a way to show expressive range

generator additional features
-------------------------

define child parameter search space relative to parent parameter value
for example the position search space of chairs based on the point coordinates of the room polygon

implicit child->parent relation given by distance metric

work with fixed shape and variable shape layout nodes
the shape depends on the variables of the parent (don estimate as joint)
the shape is a child of the node
->this is interesting for training rooms, or tables that can vary in size

apply affine transformation on gmm, to make gmm models more reusable see https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Marginal_distributions
->this means that all training of variables can be done within a unit cube, only training on the relative
relation between parent and child. And afterwards the affine transformation of (pos,rot,size) of the parent can be applied on the same gmm without having to recalulate it

make it possible to "freeze' a stochastic variable at a certain position

add method to request all stochastic variables

make it possible to switch back to original prior distribution, after learning, for a named var

allow tree structure for node variables, more generically would be to allow variables of a node be a node again

better print of node sample


code clean up
----------------------

to speed up fitness func Prepared Geometry Operations

remove colors from search space and add type

put some protection on the properties of sample nodes, in terms of querying variable name and sizes
the treedef should be use not the sample nodes

add enum to know if stochastif is discrete or continuous

remove variable classes from search_space module

change name markov tree node

add type checking in python, seriously, where necessary (see package in bookmark thesis)
Types as annotations


examples
-------------------------

different model and constraints: generate chamber in a house, this is a shape->shape relation

see paper open world layour for coffee bar exmaple

test learning with larger hierachy -> 2 level constraints (room->table_chairs)

do the plates on a table example for multiple fitness

do an example of room generation, where the fitness can be the validity of a polygon (non-convex)

