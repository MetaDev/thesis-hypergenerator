all papers in downloads->put in mendeley
compare result before and after training on the fitness

show some metrics on the computation time difference between "shape" likelihood and gameplay likelihood

1) experiment with conditional gmm 
    -make experiment to show how training 
        on filtered data is done for hard constraints
        on augmented data for fitness
        on conditional data for soft constraints
    -compare augmented data vs non-augemented data
    -compare seperate models (for each number of children) and pair-wise trained models
    -compare training on multiple fitness
    -use dimensionality reduction to find independent components->only try to find cond distr between dep components

2) model pokemon world probabilistically
    find top down constraints

3) extend mixture models to find general mixture models (include discrete distribution) (pomegranata)
    -find conditional distributions reflecting the number of children in the hierarchy conditional on the fitness

3) try to learn relation beyond parent-> child but also child->child using Hidden markov model

3) create experiment with pymc3 to be able to compare inference with and without learning
learning is applicable to top down constraints which are can do not require complex mapping (directly on the sample property)
-> show with graph
These hard constraints can however still be learned early in the generation procedure (early stop)

4) maybe experiment with dimensionality reduction


in highly constrained space, high probability for collision, objects are placed conditional on the siblings



learn conditional distributions (P(attr|index,nr_of_siblings,parent_attr)) on the independ values (of one or more attributes) of the samples, using kde or GMM
the samples of which distribution is estimated are a subset of all samples defined by a fitness criteria rejection function
this fitness rejection function defines a hyperplane (can be stochastic) that seperates valid from invalid samples

TODO: test for multiple attributes, multiple fitness , multiple models and hyperparameters

test conditional model to generate valid data conditional on index and number of siblings parent values  and train different model
for each number of children

kernel density, try to find the same conditional probabilities as gaussian mixture




save generated samples in data frame to save space


start pokemon use case


There are 2 options to make markov model
either sample first from the possible number of children combinations and use this to build a bayesian network
or add a state (markov state) for each possible number of children of a category and choose each state with given probability
and finally connect the actual child attribute distributions to each state, repeat until comple network is build

implement search space in pymc3 using 1 model for the parent child structure and when sampling build new model based on this model
these new models are cached in map based on their structure, such that each different model only has to be build once