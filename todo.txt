add experiment to show that both the fitness and conditional distributions work

work with regular formed children

add boxplot for fitness

visualise with ellipses instead of with samples


sibling fitness
---------------

check if gmm can be used for discrete random var as well (nr_siblings, index (fixed order, not index from samples))

construct seperate models for each number of children if trained on (ordered) sibling data and their fitness that is calculated between siblings
or train model on the joint of (nr_siblings, index (fixed order, not index from samples)) and parent sibling  using pomegranata (use rejection sampling for sampling, or better MCMC (PyMC!))
and train a model seperately on the structure of the network, using only (parent_n_siblings, n_siblings)->fitness to generate (conditional) n_siblings 
than use training data to estimate optimal distribution for the number of children ()

next experiment should show possibility to constraints that are enforced on siblings

increase variables with rotation and size  (add maybe covariance estimation to reduce traingin time if for example rotation doesn't affect the constrained)

start generating scenes for pokemon

use methode to sample node create from parent_sample

add method to sample node to extract data (from generated samples) which is random and thus of which the distr can be learned (for easier use), additional covariance estimation before training
s
all papers in downloads->put in mendeley


show some metrics on the computation time difference between "shape" likelihood and gameplay likelihood



1) experiment with conditional gmm 
    -make experiment to show how training 
        on filtered data is done for hard constraints
        on conditional data for soft constraints
        using weighted samples on for fitness 
    -compare seperate models (for each number of children) and pair-wise trained models
    -compare training on multiple fitness
    -use dimensionality reduction to find independent components->only try to find cond distr between dep components

2) model pokemon world probabilistically
    find top down constraints

3) extend mixture models to find general mixture models (include discrete distribution) (pomegranata)
    -find conditional distributions reflecting the number of children in the hierarchy conditional on the fitness

3) try to learn relation beyond parent-> child but also child->child using Hidden markov model

3) create experiment with pymc3 to be able to compare inference with and without learning
learning is applicable to top down constraints which are can do not require complex mapping (directly on the sample property)
-> show with graph
These hard constraints can however still be learned early in the generation procedure (early stop)

4) maybe experiment with dimensionality reduction


in highly constrained space, high probability for collision, objects are placed conditional on the siblings



learn conditional distributions (P(attr|index,nr_of_siblings,parent_attr)) on the independ values (of one or more attributes) of the samples, using kde or GMM
the samples of which distribution is estimated are a subset of all samples defined by a fitness criteria rejection function
this fitness rejection function defines a hyperplane (can be stochastic) that seperates valid from invalid samples

TODO: test for multiple attributes, multiple fitness , multiple models and hyperparameters

test conditional model to generate valid data conditional on index and number of siblings parent values  and train different model
for each number of children

kernel density, try to find the same conditional probabilities as gaussian mixture




save generated samples in data frame to save space


start pokemon use case


There are 2 options to make markov model
either sample first from the possible number of children combinations and use this to build a bayesian network
or add a state (markov state) for each possible number of children of a category and choose each state with given probability
and finally connect the actual child attribute distributions to each state, repeat until comple network is build

implement search space in pymc3 using 1 model for the parent child structure and when sampling build new model based on this model
these new models are cached in map based on their structure, such that each different model only has to be build once