set up tests to compare with weighted samples

integrate DPGMM in GMM

calculate MSE of GMM

add k fold cross validation to find best best fitted model due to stochastic EM
use different metrics, fitness MSE, fit and weighted variance of n generated samples

add gridsearch on parameters to find best parameters

reachability

learn different gmm if stochastic number of children


do some reading on gmm regression
than implement regression

truncuated gmm with MCMC

factory method to visualise fitness functions

more variables and fitness functions

learn to fit surface according to surface ratio in a polygon

find out why alignment is so hard to learn in gmm2

plot fitness seperately, fitness func is maybe not correct, try to visualise it

in training utility, a method sould receive the samples, and the fitness function and return
the fitness values and according data used to calculate it
also an argument for fitness threshold, fitness target and whether you're looking to minimize or maximize the


visualisation is part of the model, the mapping
add visualisation to fitness model as well

research:

Neufert constraints

research question: How to optimise the search space of a procedural content generator, a priori defined by bottom up constraints and a posteriori by its top-down constraints, without negatively impacting its expressiveness?

evaluation, the type of metrics and parameters that will be varied
3 models for scenes, constraints and search space


learning optimisation
-------------------------

instead of multiplying the multiple fitness functions -> dim reduction

test learning with and without threshold on the fitness

penalise gmm that strongly exceeds search space bounds (using likelyhood)

initialise weights,covars and priors such that the gmm fits a unifrom distr in the given search space
use standard sk init for this, maybe only updating the covars and setting the means on a grid and weights uniform

only assign sibling conditionallity if the found distribution is above a certain order of quality (in expressiveness)
or if higher order distribution is of lower quality, only train until n children and assign variables

evaluate quality of learned model, MSE form random points with high fitness

In learning not only the finding of optimum should be encouraged (by the log likelihood) but also expressiveness
should be promoted in the learning algorithm, that's why an important optimisation would be to additionally reward
data points based on their difference from previously used data points in the learning algorithm
any distance metric could be used for this

increase learning and sampling robustness by using k-fold cross validation.
wich would result in a mixture of GMM's with uniform weights to sample from.
or blend different mixtures together
this would increase the sampling time, but reduce the accidental bias

instead of modelling the curve fitting using density estimation (extra likelihood layer), I could do regression on the fitness
p(p,c0,c1,...ck,f)-> integral (f^1_lowerthreshold)(p(ck|p,c0,...,f))
and integrate (http://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html)
this even supports multi variable fitness, thus we could define the fitness as a joint distribution

allow training with tied covariance for the gmm, this should be reset to full after training because gmr doesnt support tied

use feature extraction to optmize learning, but each feature should be able to be reconverted

use BIC to find optimal number of clusters

normalise the data you train on [0,1] -> use affine transform of the search space

use pomegranate library

id hyperparameters for over/underfitting

make interface for training, train in batches by sampling from the previous model in the next
only expose hyperparamters that could influence over/under fitting (to be determined by experimentation)

learn(n_sample_barhces,n_batches,child_attributes,parent_attributes,fitness_funcs, GMM_hyperparams)

Note: larger batches will mean less overfitting because the model will train on more varied data
smaller batches but more number of batches will more overfit, reducing models expressivens but increasing average fitness faster


training should be done on scaled data
given only a set of attributes to train on and fitness function on the attribute vector for the weight of the samples

check if cross validation can benefit the system

experiment weighted samples vs slicing and  data or show that data with 0 weight is equal to sliced data

find attributes in children that don't interfere with fitness (can be omitted from training) (automatic feature extraction)
or attributes independent of others regardin the fitness (covariance analysis) (they can be trained seperately)(dim reduciton)

or even better (and in the case of child-child training): for large number of chidlren, deduce hierarchical independence between children (using clustering), this will return cluster label or each sample, which can be used to slice data accordingly for training
e.g. tables far apart will not likely collide
and train in a hierachical divided search space (naive bayes?, divide search space in n clusters)

train gaussian mixture using pomegranate (mixture of multivariate gaussian) (added benefit of decay, priority based multi-objective) also faster, correct see http://homes.cs.washington.edu/~jmschr/lectures/pomegranate.html

for larger number of children , the search space parent=>child should be specified implicitly
this would also allow the ordering of children to be according to fitness (implicitly)

best is to have seperate model for each number of children but to share some paremeters between the models
one such thing that I can think of is expressiveness, you want each model to be as expressive as the other
this could maybe be done by sharing the sum of covariances as parameter?


sampling optimisation
----------------------------

to sample from learned distribution using truncuated normal sampling

in your initial sampling you can always make the trade-off between expressiveness or exploration (search for diverse points)
and fitness or exploitation (search for best points)

conditioning is at the core of the sampling algorithm so that should be optimised

remove deterministic affine transformation chain from the nodesample class,
just sample the nodes and use the original node to calculate the relative values needed for the mapping

convert to more functional structure using rx, see rx_test
->use reactive programming, which makes it an data flow graph,
->make it possible to generate n samples, by creating a tree where each parent has max N_children subscriber nodes
->when sampling (2,5) number of children and 3 is calculated only send data to 3 out of children
->connect every node to an ouput node where all generated samples are captured

sample in batches using buffer of rx and train on each seperate buffer, afterwards evaluate each trained model
on average fitness and expressiveness

check which gmm samples the fastest

sampling would be greatly improved, and thus also the learning
if I would sample more (calculate fitness sample value pairs) on variables defined stochastic
even better would be to do that proportional to it's stochasity, this would be overfitting though

extend search space distribution with sample(n_samples), to generate multiple samples from the search space at once, this is an optimisation

sample in batches, resampling is done from trained model

check if data augmentation helps, extra samples where the fitness function varies a lot between close points
find high uncertainty in search space, especially around the borders, this does influence the search space (unintended complexity)

deterministic/stochastic sampling with stochastic mesh grid tree search

visualisation optimisation
----------------------------------------

calculate the surface difference between search space (rectangle) and a 99% confidence interval ellips

visualise search space of a variable, a rectangle or line for unifrom and ellipses or normal for gmm
add a method to the search space to visualise any variable by name

could add fitness in representative range visualisation (color line in gradient reflecting each value avg fitness)

when visualising the plot should show also completely the ellipses of the distributions, now they are cut off

plot, during training, the fitness histogram (avg, var), and the expressiveness (the ratio of the interval you're still reaching, or the amount of variance)

implement method to cluster (unweighted )samples in X clusters and generate N samples from each cluster
to visualise as a way to show expressive range

generator additional features
-------------------------

implicit child->parent relation given by distance metric

work with fixed shape and variable shape layout nodes
the shape depends on the variables of the parent (don estimate as joint)
the shape is a child of the node
->this is interesting for training rooms, or tables that can vary in size

apply affine transformation on gmm, to make gmm models more reusable see https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Marginal_distributions
->this means that all training of variables can be done within a unit cube, only training on the relative
relation between parent and child. And afterwards the affine transformation of (pos,rot,size) of the parent can be applied on the same gmm without having to recalulate it

make it possible to "freeze' a stochastic variable at a certain position

add method to request all stochastic variables

make it possible to switch back to original prior distribution, after learning, for a named var

allow tree structure for node variables, more generically would be to allow variables of a node be a node again

better print of node sample


code clean up
----------------------

add enum to know if stochastif is discrete or continuous

remove variable classes from search_space module

change name markov tree node

add type checking in python, seriously, where necessary (see package in bookmark thesis)
Types as annotations


examples
-------------------------

different model and constraints: generate chamber in a house, this is a shape->shape relation

see paper open world layour for coffee bar exmaple

test learning with larger hierachy -> 2 level constraints (room->table_chairs)

do the plates on a table example for multiple fitness

do an example of room generation, where the fitness can be the validity of a polygon (non-convex)

evaluation and tests
------------------
compare different values of capping the fitness
compare different values of conditions for the fitness

compare weighted samples with regression

try dimensionality reduction to convert multi-objective in single-objective (PCA)
also try weighted and multiplied fitness functions.

evaluate with and without poisson sampling, change the search space size and number of dimensions and
compare expressiveness and mse

model 3 different scenec to do the tests on

influence of model prior (hierachical relations) on learning and fitness (hierachical relations)
compare learning performance with and withour hierarchical relations (additive for pos and rot and scale for size)

this could be done by modelling search space hierachically: parent-> children
and flat parent,children and defining a parent->child relation functionaly e.g. by distance
->closest 5 chairs to table are his children
afterwards compare expressiveness, learning convergence,..

a good example would be a long search space for a forest where there needs to be a path between the trees
dividing this stroke in smaller substrokes is unintuitive for the user and will affect the results expressivens
using the implicit parent child relation would be more intuitive than explicit search space definition in this stroke
or when the user doesn't know what the search space will look like.

compare sibling learning between seperate models and full joint with marganilising
seperate models for P(c1|c0,p), P(c2|c1,c0,p),... or one model for P(c0,c1,c2,..|p) and marginilise out

compare joint learning parent->child child->child with seperate learning
seperate model for P(c|p) , P(c1|c0),p(c2|c1,c0),... and assume independende between distributions
thus we get P(c1|c0,p)=P(c1,c0,p)/p(c0,p)=(P(c1|c0)*P(c0|p))/P(c0,p)
or a single model P(c1|c0,p)

expressiveness evaluaion: difference in surface between polygon and surface of gaussians
likelihood of handmade examples in learned distribution

the fitted gaussian mixture model can be evaluated by estimating the fitness of a point and plotting
the MSE, a metric proposed in thesis about gmm regression

calculate expressivenes as the weighted average of the distance between samples, where the weight is based on the
fitness of the samples

could maybe the entropy of the gaussian as a measure of information give some form of expressiveness?


metrics
---------------------
MSE with fitness
fitness mean and variance
weighted data diversity (distance between points) weighted with fitness-> Measures of statistical dispersion


writing
---------------------

argument taxonomy of constraints based on examples found in architectural books

discuss truncuatedsampling vs truncuated learning, performance vs precision (overfitting)


sibling fitness
---------------
1. order siblings based on pair-wise fitness (weight function for samples)


2.
construct seperate models for each number of children if trained on (ordered) sibling data and their fitness that is calculated between siblings (tree structure)

and train a model seperately on the structure of the network, using only (parent_n_siblings, n_siblings)->fitness to generate (conditional) n_siblings, than use training data to estimate optimal distribution for the number of children ()


4. for a large range in the number of children (for example a large area where trees are placed but the area can be both dense or open)-> seperate models unfeasible, fit distribution for a markov chain between ordered siblings (using gmm)





seperate idea, intuitive visualisation of dimensionality of each vector would be really handy
this can be done by setting the dimensionality of the input and other necessary variables




