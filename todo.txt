refactor other code to work with new search space
test swap of variables

make interface to swap learned distribution with priorly defined distr in the search space


check how kwargs works


train parent child with child of varying size and rotation (overlap fitness), and sample from it
also train on varying size of parent, will most likely influence optimal number of children

remove variable classes from search_space module

change name markov tree node

when visualising the plot should show also completely the ellipses of the distributions, now they are cut off

add type checking in python, seriously, where necessary (see package in bookmark thesis)
Types as annotations



extend search space distribution with sample(n_samples), to generate multiple samples from the search space at once, this is an optimisation
make value a hidden attribute in variable also name, due to consistency in api between all variables

factory method to

add second constraint: train child for non overlap and alignment of child shape with the closest line of polygon of parent
first train on position, rotation,size without extra consrtaints, add later
find a list that summarizes constraints, dist based, surface based, ratios (read the book)


make interface for training, train in batches by sampling from the previous model in the next
only expose hyperparamters that could influence over/under fitting (to be determined by experimentation)
Note: larger batches will mean less overfitting because the model will train on more varied data
smaller batches but more number of batches will more overfit, reducing models expressivens but increasing average fitness faster
training should be done on scaled data
given only a set of attributes to train on and fitness function on the attribute vector for the weight of the samples


try to make learning deterministic (using random state) to be able to objectively compare between multiple runs

add low and high parameters to learned distribution


expand search space such that it is possible to have a tree structure of vars inside a node (to define functional relation between its vars), this can already be achieved by nesting vectorvariables, only thing to add is to pass upper level
variables within same node as well
----------------------------------------
check if cross validation can benefit the system

experiment weighted samples vs slicing and  data or show that data with 0 weight is equal to sliced data

check if data augmentation helps, extra samples where the fitness function varies a lot between close points
find high uncertainty in search space, especially around the borders, this does influence the search space (unintended complexity)

find attributes in children that don't interfere with fitness (can be omitted from training) (automatic feature extraction)
or attributes independent of others regardin the fitness (covariance analysis) (they can be trained seperately)(dim reduciton)

or even better (and in the case of child-child training): for large number of chidlren, deduce hierarchical independence between children (using clustering), this will return cluster label or each sample, which can be used to slice data accordingly for training
e.g. tables far apart will not likely collide
and train in a hierachical divided search space (naive bayes?, divide search space in n clusters)

train gaussian mixture using pomegranate (mixture of multivariate gaussian) (added benefit of decay, priority based multi-objective) also faster (according to author)

different model and constraints: generate chamber in a house, this is a shape->shape relation

plot, during training, the fitness histogram (avg, var), and the expressiveness (the ratio of the interval you're still reaching, or the amount of variance)


define constraints as either a hard,soft constraint , or a variable constraint
check what works best for soft and hard constraints: slice/filter data or use weighted samples
the variable constraint acts as condition on the to generate data

refactor distribution class, to be able to use either uniform (unknow density) (or other scipy stats distr.) or mix of gaussians (trained density)
support shape (for random vector) in distribution



test learning with larger hierachy -> 2 level constraints (room->table_chairs)


next experiment should show possibility to constraints that are enforced on siblings
sibling fitness
---------------
1. order siblings based on pair-wise fitness (weight function for samples)

2. train HMM on sequence of (parent,child1),(parent,child2), to be able to calculate conditional distr
afterwards

2.
construct seperate models for each number of children if trained on (ordered) sibling data and their fitness that is calculated between siblings (tree structure)

and train a model seperately on the structure of the network, using only (parent_n_siblings, n_siblings)->fitness to generate (conditional) n_siblings, than use training data to estimate optimal distribution for the number of children ()

3.
or train model on the joint of (nr_siblings, index (fixed order, not index from samples)) and parent  using pomegranata (use rejection sampling for sampling, or better MCMC (PyMC!))

4. for a large range in the number of children (for example a large area where trees are placed but the area can be both dense or open)-> seperate models unfeasible, fit distribution for a markov chain between ordered siblings (using gmm)

or better with a Hidden markov model using the children attributes as sequences
the hmm library is either hmmlearn or bnpy (use weighted sampels), propably won't integrate well with hierarchical model, set number of states the max(n_children) initialise transation distr according to specification in search space if the distr of n_children should not be learned
how do we condition on the parent state, convert the model to pymc and set the first state to observed. we could add the parent to the start of the sequence, but this would reduce our hierarchical model to a single long chain, not if we can condition on the first state




add method to sample node to extract data (from generated samples) which is random and thus of which the distr can be learned (for easier use), additional covariance estimation before training
s

all papers in downloads->put in mendeley


show some metrics on the computation time difference between "shape" likelihood and gameplay likelihood


    -compare seperate models (for each number of children) and pair-wise trained models
    -compare training on multiple fitness
    -use dimensionality reduction to find independent components->only try to find cond distr between dep components

2) model pokemon world probabilistically
    find top down constraints
find conditional distributions reflecting the number of children in the hierarchy conditional on the fitness

3) try to learn relation beyond parent-> child but also child->child using Hidden markov model



