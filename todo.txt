show fitness before and after training

train parent child with child of varying size and rotation (overlap fitness), and sample from it

add second constraint: train child for non overlap and closeness to corners of polygon

----------------------------------------

experiment weighted samples vs slicing and  data or show that data with 0 weight is equal to sliced data

check if data augmentation helps, extra samples where the fitness function varies a lot between close points
find high uncertainty in search space, especially around the borders

find attributes in children that don't interfere with fitness (covariance analysis)
or even better (and in the case of child-child training): for large number of chidlren, deduce hierarchical independence between children
e.g. tables far apart will not likely collide
and train in a hierachical divided search space (naive bayes?, divide search space in n clusters)

train gaussian mixture using pomegranate (added benefit of decay, priority based multi-objective)

different model and constraints: generate chamber in a house, this is a shape->shape relation

plot, during training, the fitness histogram (avg, var), and the expressiveness (the ratio of the interval you're still reaching, or the amount of variance)

find a list that summarizes constraints, dist based, surface based, ratios,

define constraints as either a hard,soft constraint , or a variable constraint
check what works best for soft and hard constraints: slice/filter data or use weighted samples
the variable constraint acts as condition on the to generate data

refactor distribution class, to be able to use either uniform (unknow density) (or other scipy stats distr.) or mix of gaussians (trained density)
support shape (for random vector) in distribution

factory method to generate new hierarchical samples after training, by automatically sampling from trained and untrained distributions
given only a set of attributes to train on and fitness function on the attribute vector for the weight of the samples

test learning with larger hierachy -> 2 level constraints (room->table_chairs)


sibling fitness
---------------

-1. don't do (try it out anyway)
check if gmm can be used for discrete random var as well (nr_siblings, index (fixed order, not index from samples))
No it can't and it shouldn't for discrete variables you should use binomial

2.
construct seperate models for each number of children if trained on (ordered) sibling data and their fitness that is calculated between siblings (tree structure)

and train a model seperately on the structure of the network, using only (parent_n_siblings, n_siblings)->fitness to generate (conditional) n_siblings, than use training data to estimate optimal distribution for the number of children ()

3.
or train model on the joint of (nr_siblings, index (fixed order, not index from samples)) and parent  using pomegranata (use rejection sampling for sampling, or better MCMC (PyMC!))

4. for a large range in the number of children (for example a large area where trees are placed but the area can be both dense or open)-> seperate models unfeasible, fit distribution for a markov chain between ordered siblings (using gmm)

or better with a Hidden markov model using the children attributes as sequences
the hmm library is either hmmlearn or bnpy (use weighted sampels), propably won't integrate well with hierarchical model, set number of states the max(n_children) initialise transation distr according to specification in search space if the distr of n_children should not be learned
how do we condition on the parent state, convert the model to pymc and set the first state to observed. we could add the parent to the start of the sequence, but this would reduce our hierarchical model to a single long chain, not if we can condition on the first state

next experiment should show possibility to constraints that are enforced on siblings

increase variables with rotation and size  (add maybe covariance estimation to reduce traingin time if for example rotation doesn't affect the constrained)

start generating scenes for pokemon

use methode to sample node create from parent_sample

add method to sample node to extract data (from generated samples) which is random and thus of which the distr can be learned (for easier use), additional covariance estimation before training
s
all papers in downloads->put in mendeley


show some metrics on the computation time difference between "shape" likelihood and gameplay likelihood



1) experiment with conditional gmm
    -make experiment to show how training
        on filtered data is done for hard constraints
        on conditional data for soft constraints
        using weighted samples on for fitness
    -compare seperate models (for each number of children) and pair-wise trained models
    -compare training on multiple fitness
    -use dimensionality reduction to find independent components->only try to find cond distr between dep components

2) model pokemon world probabilistically
    find top down constraints

3) extend mixture models to find general mixture models (include discrete distribution) (pomegranata)
    -find conditional distributions reflecting the number of children in the hierarchy conditional on the fitness

3) try to learn relation beyond parent-> child but also child->child using Hidden markov model

3) create experiment with pymc3 to be able to compare inference with and without learning
learning is applicable to top down constraints which are can do not require complex mapping (directly on the sample property)
-> show with graph
These hard constraints can however still be learned early in the generation procedure (early stop)

4) maybe experiment with dimensionality reduction


in highly constrained space, high probability for collision, objects are placed conditional on the siblings



learn conditional distributions (P(attr|index,nr_of_siblings,parent_attr)) on the independ values (of one or more attributes) of the samples, using kde or GMM
the samples of which distribution is estimated are a subset of all samples defined by a fitness criteria rejection function
this fitness rejection function defines a hyperplane (can be stochastic) that seperates valid from invalid samples

TODO: test for multiple attributes, multiple fitness , multiple models and hyperparameters

test conditional model to generate valid data conditional on index and number of siblings parent values  and train different model
for each number of children

kernel density, try to find the same conditional probabilities as gaussian mixture




save generated samples in data frame to save space


start pokemon use case


There are 2 options to make markov model
either sample first from the possible number of children combinations and use this to build a bayesian network
or add a state (markov state) for each possible number of children of a category and choose each state with given probability
and finally connect the actual child attribute distributions to each state, repeat until comple network is build

implement search space in pymc3 using 1 model for the parent child structure and when sampling build new model based on this model
these new models are cached in map based on their structure, such that each different model only has to be build once